{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Sycophancy Analysis - Part 1: Data Loading & Setup\n",
    "\n",
    "This notebook loads the combined responses JSON and sets up the analysis pipeline.\n",
    "\n",
    "## Overview\n",
    "- Load combined responses from `results/combined_run_0c_1_1b/responses_combined.json`\n",
    "- Score responses using `build_sss()` to compute sycophancy metrics\n",
    "- Generate basic statistics and data quality checks\n",
    "- Prepare data for downstream analysis notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append('..')\n",
    "from sycophancy_analysis.scoring.sss import build_sss\n",
    "from utils.score_metrics_from_combined import score_and_metrics\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Combined Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the combined responses JSON\n",
    "data_path = Path('../results/combined_run_0c_1_1b/responses_combined.json')\n",
    "\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    responses_data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(responses_data)\n",
    "print(f\"Loaded {len(df)} responses\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Quality Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"=== Data Quality Summary ===\")\n",
    "print(f\"Total responses: {len(df)}\")\n",
    "print(f\"Unique models: {df['model'].nunique()}\")\n",
    "print(f\"Unique prompts: {df['prompt_id'].nunique()}\")\n",
    "print(f\"Source runs: {df['source_run'].value_counts().to_dict()}\")\n",
    "\n",
    "print(\"\\n=== Missing Values ===\")\n",
    "missing = df.isnull().sum()\n",
    "missing = missing[missing > 0].sort_values(ascending=False)\n",
    "print(missing)\n",
    "\n",
    "print(\"\\n=== Model Distribution ===\")\n",
    "model_counts = df['model'].value_counts()\n",
    "print(model_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Score Responses with SSS Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the scoring utility to compute all metrics\n",
    "print(\"Computing sycophancy metrics...\")\n",
    "output_files = score_and_metrics(\n",
    "    input_path='../results/combined_run_0c_1_1b/responses_combined.json',\n",
    "    output_prefix='../results/combined_run_0c_1_1b',\n",
    "    pretty=True\n",
    ")\n",
    "\n",
    "print(\"Generated files:\")\n",
    "for name, path in output_files.items():\n",
    "    print(f\"  {name}: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Scored Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the scored responses\n",
    "with open('../results/combined_run_0c_1_1b/scored_rows.json', 'r') as f:\n",
    "    scored_data = json.load(f)\n",
    "\n",
    "scored_df = pd.DataFrame(scored_data)\n",
    "print(f\"Scored {len(scored_df)} responses\")\n",
    "print(f\"SSS Metrics columns: {[c for c in scored_df.columns if c not in df.columns]}\")\n",
    "\n",
    "# Load delta by topic\n",
    "with open('../results/combined_run_0c_1_1b/delta_by_topic.json', 'r') as f:\n",
    "    delta_data = json.load(f)\n",
    "\n",
    "delta_df = pd.DataFrame(delta_data)\n",
    "print(f\"\\nDevil's Advocate Delta: {len(delta_df)} model-topic pairs\")\n",
    "\n",
    "# Load strength curves\n",
    "with open('../results/combined_run_0c_1_1b/strength_curves.json', 'r') as f:\n",
    "    curves_data = json.load(f)\n",
    "\n",
    "print(f\"Strength curves for {len(curves_data)} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Quick-Look Executive Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate executive summary metrics\n",
    "print(\"=== QUICK-LOOK EXECUTIVE SUMMARY ===\")\n",
    "\n",
    "if 'flattery' in scored_df.columns:\n",
    "    # Highest unconditional flattery\n",
    "    flat_by_model = scored_df.groupby('model')['flattery'].mean().sort_values(ascending=False)\n",
    "    print(f\"• Highest unconditional flattery: {flat_by_model.index[0]} (FLAT = {flat_by_model.iloc[0]:.3f})\")\n",
    "\n",
    "if 'harm_validation' in scored_df.columns:\n",
    "    # Lowest harm validation\n",
    "    hvs_by_model = scored_df.groupby('model')['harm_validation'].mean().sort_values()\n",
    "    print(f\"• Lowest harm validation: {hvs_by_model.index[0]} (HVS = {hvs_by_model.iloc[0]:.3f})\")\n",
    "\n",
    "# Sharpest stance-elasticity from curves\n",
    "max_ae = 0\n",
    "max_ae_model = \"\"\n",
    "for model, curves in curves_data.items():\n",
    "    if 'regular' in curves and 'AE_standardized' in curves['regular']:\n",
    "        ae = curves['regular']['AE_standardized']\n",
    "        if ae > max_ae:\n",
    "            max_ae = ae\n",
    "            max_ae_model = model\n",
    "\n",
    "if max_ae_model:\n",
    "    print(f\"• Sharpest stance-elasticity: {max_ae_model} (AE slope = {max_ae:.3f})\")\n",
    "\n",
    "# Most significant delta\n",
    "if not delta_df.empty and 'delta' in delta_df.columns:\n",
    "    max_delta_row = delta_df.loc[delta_df['delta'].idxmax()]\n",
    "    print(f\"• Largest Devil's Advocate resistance: {max_delta_row['model']} on {max_delta_row['topic']} (Δ = {max_delta_row['delta']:.3f})\")\n",
    "\n",
    "print(f\"\\n• Total models analyzed: {scored_df['model'].nunique()}\")\n",
    "print(f\"• Total prompts per model: {scored_df['prompt_id'].nunique()}\")\n",
    "print(f\"• Topics covered: {', '.join(scored_df['topic'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Export for Analysis Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed dataframes for other notebooks\n",
    "scored_df.to_json('../results/combined_run_0c_1_1b/scored_responses_processed.json', orient='records', indent=2)\n",
    "delta_df.to_json('../results/combined_run_0c_1_1b/delta_processed.json', orient='records', indent=2)\n",
    "\n",
    "print(\"Exported processed data for downstream analysis:\")\n",
    "print(\"- scored_responses_processed.json\")\n",
    "print(\"- delta_processed.json\")\n",
    "print(\"- strength_curves.json (already available)\")\n",
    "\n",
    "print(\"\\n=== Ready for Analysis! ===\")\n",
    "print(\"Next notebooks:\")\n",
    "print(\"- 02_stance_elasticity_analysis.ipynb\")\n",
    "print(\"- 03_harm_validation_profiling.ipynb\")\n",
    "print(\"- 04_clustering_and_comparison.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
